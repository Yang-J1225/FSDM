model:
  target: models.unet.UNetModelSwin
  ckpt_path: /home/ysj/ResShiftV2/resshift_finetune/2024-09-12-14-01/ema_ckpts/ema_2000.pth
  params:
    image_size: 64
    in_channels: 6
    model_channels: 160
    out_channels: 3
    cond_lq: True
    attention_resolutions: [64,32,16,8]
    dropout: 0
    channel_mult: [1, 2, 2, 4]
    num_res_blocks: [2, 2, 2, 2]
    conv_resample: True
    dims: 2
    use_fp16: False
    num_head_channels: 32
    use_scale_shift_norm: True
    resblock_updown: False
    swin_depth: 2
    swin_embed_dim: 192
    window_size: 8
    mlp_ratio: 4
    use_flow: False
    window_size: 3
    
flownet:
  target: core.raft.RAFT
  ckpt_path: weights/raft-things.pth
  params:
      pretrained: weights/raft-things.pth
# flownet:
#   target: models.spynet.SPyNet
#   ckpt_path: weights/spynet_20210409-c6c1bd09.pth
#   params:
#       pretrained: weights/spynet_20210409-c6c1bd09.pth
diffusion:
  target: models.script_util.create_gaussian_diffusion
  params:
    sf: 4
    schedule_name: exponential
    schedule_kwargs:
      power: 0.3
    etas_end: 0.99
    steps: 15
    min_noise_level: 0.04
    kappa: 1.0
    weighted_mse: False
    predict_type: xstart
    timestep_respacing: ~
    scale_factor: 1.0
    normalize_input: True
    latent_flag: True

autoencoder:
  target: ldm.models.autoencoder.VQModelTorch
  ckpt_path: weights/autoencoder_vq_f4.pth
  use_fp16: True
  params:
    embed_dim: 3
    n_embed: 8192
    ddconfig:
      double_z: False
      z_channels: 3
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
      - 1
      - 2
      - 4
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
      padding_mode: zeros

data:
  train:
    type: reds
    params:
      sf: 4
      video_folder: /media/amax/chenchao/REDS/train/train_sharp
      sample_stride: 1
      sample_n_frames: 16
      mean: 0.5
      std: 0.5
      hflip: False
      rotation: False
      need_path: False
      gt_size: 256
  val:
    type: reds
    params:
      sf: 4
      video_folder: /media/amax/chenchao/REDS/val/val_sharp
      sample_stride: 1
      sample_n_frames: 16
      mean: 0.5
      std: 0.5
      hflip: False
      rotation: False
      need_path: False
      gt_size: 256
      length: 10
      val_mode: True
#  train:
#    type: webvid
#    params:
#      sf: 4
#      csv_path: /media/amax/chenchao/webvid/results_2M_train.csv
#      video_folder: /media/amax/chenchao/webvid/2M_train/videos
#      sample_stride: 2
#      sample_n_frames: 16
#      mean: 0.5
#      std: 0.5
#      hflip: False
#      rotation: False
#      need_path: False
#      gt_size: 256
#  val:
#    type: webvid
#    params:
#      sf: 4
#      csv_path: /media/amax/chenchao/webvid/results_2M_val.csv
#      video_folder: /media/amax/chenchao/webvid/2M_val/videos
#      sample_stride: 2
#      sample_n_frames: 16
#      mean: 0.5
#      std: 0.5
#      hflip: False
#      rotation: False
#      need_path: False
#      gt_size: 256
#      length: 10
#      val_mode: True

train:
  lr: 5e-5
  batch: [12, 1]   # batchsize for training and validation
  use_fp16: False
  microbatch: 1
  seed: 123456
  global_seeding: False
  prefetch_factor: 4
  num_workers: 1
  ema_rate: 0.999
  iterations: 3000 # 500000
  milestones: [50, 500, 1000, 2000,3000]
  # milestones: [25,250, 500, 1000,3000]
  weight_decay: 0
  save_freq: 1000
  val_freq: 500001 # 目前不支持边训练边验证，所以把 val_freq 改成大于 iterations
  log_freq: [10, 500001, 500001] #[training loss, training images, val images]
  save_images: False  # save the images of tensorboard logging
  use_ema_val: False
  
  loss_coef: [1.0, 1.0]         # [mse, lpips]
  # training setting
  use_amp: False                # amp training
  compile:
    flag: True
    mode: reduce-overhead    
